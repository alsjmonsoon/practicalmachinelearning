###title: "Activity recognition and prediciton of weighting lifting exercise "
date: "June 17, 2016"
output: html document

#### summary: Data from accelerometers on belt, forearm, arm and dumbbell of 6 participants are collected when they were doing weight lifting exercises. In this report, I built predictive models to access the quality (5 classes) performed on this exercise and investigate whether we could determine qualitative activity recognitions of weight lift exercises.

#### Download, load, and clean data
Six participates were asked to perform on set of 10 repetitions of the _Unilateral Dumbbell Biceps Curl_ in five different fashions:
        ..* Class A: exactly according to the specification
        ..* Class B: throwing the elbows to the front
        ..* Class C: lifting the dumbbell only halfway
        ..* Class D: lowering the dumbbell only halfway
        ..* Class E: throwing the hips to the front
        
For data recordig four 9 degrees of freedom Razor inertial measurement units (IMU), which provide the three-axes acceleration, gyroscope and magnetometer data at a joint sampling rate of 45Hz.


Each IMU had following sensors:
        * Accelerometer(X,Y,Z). Measures accelerations, changes in positions and orientation sensor in the UP-DOWN plane.
        * Gyroscope (X,Y,Z). Measures either changes in orientation or rotational velocity.
        * Magnetometer (X,Y,Z). Measures magnetic fields, used as a compass to determine north, south, east and west orientation.
        
The IUM were mounted at: Arm, forearm, belt and dumbbell.

For feature extraction a sliding window approach with different lengths from 0.5 second to 2.5 seconds, with 0.5 second overlap. In each step of the sliding window approach they calculated features on the Euler angles (roll, pitch and yaw), as well as the raw accelerometer, gyroscope and magnetometer readings. For the Euler angles of each of the four sensors we calculated eight features: mean, variance, standard deviation, max, min, amplitude, kurtosis and skewness.

Total number of measured variables: 160

        * 4 IMU's times of Acc(x,y,z), Gyro(x,y,z), Magno(x,y,z) = 36
        * 4 Roll, Pitch, Yaw derived values (29) = 116
        * ID, Subject, Time related variables = 7
        * Classe = 1

```{r load_data, warning=F, message=FALSE,cache=TRUE, echo=TRUE, message=FALSE}
# fileUrl1<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
# pml_training<-download.file(fileUrl1,destfile="~/Documents/OneDrive/DataSci/MachineLearning/pml_training.csv", method="curl")
# fileUrl2<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
# pml_testing<-download.file(fileUrl2,destfile="~/Documents/OneDrive/DataSci/MachineLearning/pml_testing.csv", method="curl")
training<-read.csv("~/Documents/OneDrive/DataSci/MachineLearning/pml_training.csv",header=T,na.strings=c("","NA","NULL"))
testing<-read.csv("~/Documents/OneDrive/DataSci/MachineLearning/pml_testing.csv",header=T,na.strings=c("","NA","NULL"))
dim(training);dim(testing); # 'str(training[,1:10])' to see part of the columns
# get rid of the columns with any NAs
trainingNoNAs<-training[, colSums(is.na(training))==0]
# alternatively we can remove columns with more than 95% of NA or "" values
# threshold <- dim(trainingData)[1] * 0.95
# goodColumns <- !apply(trainingData, 2, function(x) sum(is.na(x)) > threshold  || sum(x=="") > threshold)
# trainingData <- trainingData[, goodColumns]

# The investigation is focused on determining whether an assessment can be made based on specific measurements of execution of a set of activities. Therefore, features used in the analysis are non-user, none-time, none-sensor specific features. remove all the time variables and columns which doesn't correlate with the "classe" variable.
finaltrain<-trainingNoNAs[,-c(1,2,3,4,5,6,7)]
dim(finaltrain)
# partition the training data into training and validation dataset, so we can test on validation dataset
```

#### Preprocessing data to removing near zero covariate, identifying correlated parameters and to center and scale the data

```{r,warning=F, message=FALSE,cache=TRUE, echo=TRUE, message=FALSE}
# removing near zero covariate if any
library(caret)
nzv<-nearZeroVar(finaltrain, saveMetric=TRUE)
filteredTrain<-finaltrain[,nzv$nzv==FALSE]
# dim(filteredTrain) 
# class(filteredTrain$classe)

# identifying correlated Predictors
trainingCor<-cor(filteredTrain[,-53])

# look at which of these correlation (which columns) is greater than 0.75
highlyCorrelated<-findCorrelation(trainingCor,cutoff=0.75)
Train<-filteredTrain[,-highlyCorrelated]
filteredCorrelation<-cor(Train[,-32])
summary(filteredCorrelation[upper.tri(filteredCorrelation)])

# split the data into training data and validation data. The validation data is used to check the accuray of the model based on training data.
set.seed(160610)
trainIndex<-createDataPartition(Train$classe,p=0.75,list=FALSE)
trainingSet<-Train[trainIndex,]
validation<-Train[-trainIndex,]

# Centering and Scaling the both training and valdiation data
preProcValues<-preProcess(trainingSet[,-32],method=c("center","scale"))
trainCS<-predict(preProcValues,trainingSet)
validationCS<-predict(preProcValues,validation)

# head(trainCS[,20:32]) --check data
```

#### Model selection and tuning
_Develop training model using random forest model(random forest model takes about 26min to run on my system using caret::train). By using random forest model, the cross validation is automatically._

```{r warning=F, message=FALSE,cache=TRUE, echo=TRUE, message=FALSE}
x <- trainCS[,-32]
y <- trainCS[,32]

#Configure parallel processing to improve the runtime performance
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

# Configure trainControl object. the number that specifies the quantity of folds for k-fold cross-validation
fitControl <- trainControl(method = "cv",
                           number = 10,
                           repeats=10,
                           allowParallel = TRUE)

fit <- train(x,y,data=trainCS,method="rf",trControl =fitControl)

# De-register parallel processing cluster
stopCluster(cluster)
```

#### assesssing the suitablity of the random forest model
```{r warning=F, message=FALSE,cache=TRUE, echo=TRUE, message=FALSE}
# accessing OBB error rate, note the error is very small, suggesting the model is good one
print(fit$finalModel) # fit$resample; confusionMatrix.train(fit)


# predict on the validation data set 
pred.fit<-predict(fit,validationCS) # default, type="class"
# measure model performance
confusionMatrix(pred.fit,validationCS[,32])
#  alternatively, we could calcuate the OOB rate using
OOB_err_rate=sum(pred.fit!=valdiationCS[,32])/length(validationCS[,32])
OOB_err_rate
# varImp(fit)  # to show the most important variables

# grab the same predicator variables for the 20 test cases
f<-c(colnames(Train[,-32]))
testCase<-testing[,f]

# because I have pre-processed the training and valdiation data by "center" and "scale" them, therefore I also preProcess the 20 test cases (by centering and scaling) the same way from above
testCS<-predict(preProcValues,testCase)
pred_1<-predict(fit,testCS)

# alternative using following code to generate answers for the test case:
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(testCS)

# I got 100% for the final quiz, suggesting the model performs well.

```
  
_Note, just to be sure, I also tested the GBM model, this takes about 15 min to run with multi-thread on my system and the accuracy is about 98.7% and it is less accurate compared to the random forest model. The R code is following_:  

```{r}
# establishing gbm model (the code for this assignment here is inactive to save HTML knit time), note, we need use the same paralle processing as above to improve runtime performance
#set.seed(825)
#gbmFit1<-train(x,y,method="gbm",trControl =fitControl,verbose=FALSE)
#gbmFit1
#gbmFit1$resample
#confusionMatrix.train(gbmFit1)
#pred.gbm<-predict(gbmFit1,validationCS[,-32])
#head(validationCS[,20:32])
#confusionMatrix(pred.gbm,validationCS[,32])
```

_Also note, I also tried the linear discriminant analysese model, the accuracy is the lowest. The code is following :_  
```{r}
# establishing lda model (the code for this assignment here is inactive to save HTML knit time). 
#set.seed(160616)
# lda.fit<-train(x,y,method="lda")
# pred.lda <- predict(lda.fit,validationCS[,-32])
# confusionMatrix(pred.lda,validationCS[,32]) # the accuracy is much lower than random forest and gradient boosting model.
```

#### Conclusion: It is possible to recognize and classify how well the activity were performed by using a random forest model. This model performs the best compared to gbm and lda models on the validation data set. The model is able to correctly predict the classes on test cases.

#### Citation  
The data for this project come from [this source](http://groupware.les.inf.puc-rio.br/har]).
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

#### Acknowledgement  
Special thanks go to Leonard Greski, the Mentor of Practical Machine Learning for his article on "Improving Performance of Random Forest in caret::train()"


